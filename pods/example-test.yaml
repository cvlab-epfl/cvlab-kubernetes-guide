apiVersion: v1
kind: Pod
metadata:
  name: username-example-test
  labels:
    user: set-your-username
spec:
  restartPolicy: Never
  containers:
    - name: base-test
      image: ic-registry.epfl.ch/cvlab/lis/lab-python-ml:py38src
      imagePullPolicy: Always # load the newest version of the image
      
      command: ["/opt/lab/setup_and_wait.sh"] 
      #   this one will do nothing and wait, so you can enter the container yourself with
      #   kubectl exec -it pod_name /bin/bash

      env:
      - name: CLUSTER_USER
        value: "username" # set this
      - name: CLUSTER_USER_ID
      # set this, run `id` on cvlab cluster to get the number, it will print among other things:
      # uid=number(yourname)
        value: "12345" 
      - name: CLUSTER_GROUP_NAME
        value: "CVLAB-unit"
      - name: CLUSTER_GROUP_ID
        value: "11166"
      # - name: SSH_PUBLIC_KEY # ssh does not seem to work yet
      #   value: "[...]"
          
      volumeMounts:
        - mountPath: /cvlabsrc1
          name: cvlabsrc1
        - mountPath: /cvlabdata1
          name: cvlabdata1
        - mountPath: /cvlabdata2
          name: cvlabdata2
        - mountPath: /dev/shm
          name: dshm

      # specify that it uses a GPU!
      # resources:
      #   limits:
      #     nvidia.com/gpu: 1 # requesting 1 GPU
  
  volumes:
    - name: cvlabsrc1
      persistentVolumeClaim:
        claimName: pv-cvlabsrc1

    - name: cvlabdata1
      persistentVolumeClaim:
        claimName: pv-cvlabdata1

    - name: cvlabdata2
      persistentVolumeClaim:
        claimName: pv-cvlabdata2

    # shared memory, often needed by PyTorch dataloaders
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 8Gi
